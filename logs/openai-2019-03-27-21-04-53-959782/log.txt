Logging to logs/openai-2019-03-27-21-04-53-959782
 the network use 2200375.0 varibales
Loaded model from trained_models/carla-dqn-model-2019-03-27-15.ckpt
--------------------------------
| episodes          | 20       |
| mean_100ep_reward | 1.22     |
| steps             | 1019     |
| time_exploring    | 90       |
--------------------------------
Saving model due to mean reward increase: None -> 1.22
--------------------------------
| episodes          | 40       |
| mean_100ep_reward | -1.5     |
| steps             | 2039     |
| time_exploring    | 80       |
--------------------------------
--------------------------------
| episodes          | 60       |
| mean_100ep_reward | -2.09    |
| steps             | 3059     |
| time_exploring    | 70       |
--------------------------------
--------------------------------
| episodes          | 80       |
| mean_100ep_reward | -3.79    |
| steps             | 4079     |
| time_exploring    | 60       |
--------------------------------
--------------------------------
| episodes          | 100      |
| mean_100ep_reward | -4.74    |
| steps             | 5099     |
| time_exploring    | 50       |
--------------------------------
--------------------------------
| episodes          | 120      |
| mean_100ep_reward | -8.12    |
| steps             | 6119     |
| time_exploring    | 40       |
--------------------------------
--------------------------------
| episodes          | 140      |
| mean_100ep_reward | -9.95    |
| steps             | 7139     |
| time_exploring    | 30       |
--------------------------------
--------------------------------
| episodes          | 160      |
| mean_100ep_reward | -7.71    |
| steps             | 8149     |
| time_exploring    | 20       |
--------------------------------
--------------------------------
| episodes          | 180      |
| mean_100ep_reward | -0       |
| steps             | 9069     |
| time_exploring    | 11       |
--------------------------------
--------------------------------
| episodes          | 200      |
| mean_100ep_reward | 11.7     |
| steps             | 9880     |
| time_exploring    | 3        |
--------------------------------
Saving model due to mean reward increase: 1.22 -> 11.68
--------------------------------
| episodes          | 220      |
| mean_100ep_reward | 28.2     |
| steps             | 10568    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 11.68 -> 28.23
--------------------------------
| episodes          | 240      |
| mean_100ep_reward | 44.1     |
| steps             | 11235    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 28.23 -> 44.06
--------------------------------
| episodes          | 260      |
| mean_100ep_reward | 54.1     |
| steps             | 11980    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 44.06 -> 54.13
--------------------------------
| episodes          | 280      |
| mean_100ep_reward | 61.7     |
| steps             | 12666    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 54.13 -> 61.69
--------------------------------
| episodes          | 300      |
| mean_100ep_reward | 65       |
| steps             | 13373    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 61.69 -> 64.95
--------------------------------
| episodes          | 320      |
| mean_100ep_reward | 66.2     |
| steps             | 14011    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 64.95 -> 66.17
--------------------------------
| episodes          | 340      |
| mean_100ep_reward | 64.7     |
| steps             | 14792    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 360      |
| mean_100ep_reward | 64.5     |
| steps             | 15572    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 380      |
| mean_100ep_reward | 63.2     |
| steps             | 16294    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 400      |
| mean_100ep_reward | 62.7     |
| steps             | 16989    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 420      |
| mean_100ep_reward | 59.8     |
| steps             | 17744    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 440      |
| mean_100ep_reward | 60.7     |
| steps             | 18454    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 460      |
| mean_100ep_reward | 62.5     |
| steps             | 19171    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 480      |
| mean_100ep_reward | 62.1     |
| steps             | 19920    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 500      |
| mean_100ep_reward | 63.4     |
| steps             | 20587    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 520      |
| mean_100ep_reward | 63.6     |
| steps             | 21305    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 540      |
| mean_100ep_reward | 64.9     |
| steps             | 21965    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 560      |
| mean_100ep_reward | 64.9     |
| steps             | 22666    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 580      |
| mean_100ep_reward | 66.8     |
| steps             | 23330    |
| time_exploring    | 2        |
--------------------------------
Saving model due to mean reward increase: 66.17 -> 66.75
--------------------------------
| episodes          | 600      |
| mean_100ep_reward | 65.2     |
| steps             | 24044    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 620      |
| mean_100ep_reward | 65.4     |
| steps             | 24777    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 640      |
| mean_100ep_reward | 66       |
| steps             | 25442    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 660      |
| mean_100ep_reward | 66.5     |
| steps             | 26100    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 680      |
| mean_100ep_reward | 64.1     |
| steps             | 26860    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 700      |
| mean_100ep_reward | 65       |
| steps             | 27554    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 720      |
| mean_100ep_reward | 64.5     |
| steps             | 28302    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 740      |
| mean_100ep_reward | 61.9     |
| steps             | 29058    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 760      |
| mean_100ep_reward | 62.1     |
| steps             | 29719    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 780      |
| mean_100ep_reward | 65.5     |
| steps             | 30366    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 800      |
| mean_100ep_reward | 64.6     |
| steps             | 31076    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 820      |
| mean_100ep_reward | 64.1     |
| steps             | 31880    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 840      |
| mean_100ep_reward | 65.9     |
| steps             | 32570    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 860      |
| mean_100ep_reward | 63.6     |
| steps             | 33331    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 880      |
| mean_100ep_reward | 61.6     |
| steps             | 34028    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 900      |
| mean_100ep_reward | 62.2     |
| steps             | 34726    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 920      |
| mean_100ep_reward | 62.9     |
| steps             | 35446    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 940      |
| mean_100ep_reward | 63.7     |
| steps             | 36111    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 960      |
| mean_100ep_reward | 66.1     |
| steps             | 36809    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 980      |
| mean_100ep_reward | 66.1     |
| steps             | 37513    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1000     |
| mean_100ep_reward | 65.6     |
| steps             | 38216    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1020     |
| mean_100ep_reward | 65.9     |
| steps             | 38914    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1040     |
| mean_100ep_reward | 63.1     |
| steps             | 39634    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1060     |
| mean_100ep_reward | 62.4     |
| steps             | 40342    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1080     |
| mean_100ep_reward | 64.1     |
| steps             | 40997    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1100     |
| mean_100ep_reward | 64.6     |
| steps             | 41681    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1120     |
| mean_100ep_reward | 64.9     |
| steps             | 42390    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1140     |
| mean_100ep_reward | 65.7     |
| steps             | 43086    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1160     |
| mean_100ep_reward | 64.2     |
| steps             | 43827    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1180     |
| mean_100ep_reward | 63.1     |
| steps             | 44524    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1200     |
| mean_100ep_reward | 63.9     |
| steps             | 45182    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1220     |
| mean_100ep_reward | 64.3     |
| steps             | 45898    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1240     |
| mean_100ep_reward | 62.2     |
| steps             | 46701    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1260     |
| mean_100ep_reward | 63.2     |
| steps             | 47409    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1280     |
| mean_100ep_reward | 63.1     |
| steps             | 48093    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1300     |
| mean_100ep_reward | 62.8     |
| steps             | 48759    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1320     |
| mean_100ep_reward | 62.7     |
| steps             | 49431    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 1340     |
| mean_100ep_reward | 63.9     |
| steps             | 50168    |
| time_exploring    | 2        |
--------------------------------
