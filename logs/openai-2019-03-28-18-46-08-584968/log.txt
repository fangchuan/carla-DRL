Logging to logs/openai-2019-03-28-18-46-08-584968
 the network use 2200375.0 varibales
--------------------------------
| episodes          | 20       |
| mean_100ep_reward | -2.92    |
| steps             | 890      |
| time_exploring    | 91       |
--------------------------------
Saving model due to mean reward increase: None -> -2.92
--------------------------------
| episodes          | 40       |
| mean_100ep_reward | -1.79    |
| steps             | 1566     |
| time_exploring    | 84       |
--------------------------------
Saving model due to mean reward increase: -2.92 -> -1.79
--------------------------------
| episodes          | 60       |
| mean_100ep_reward | -3.54    |
| steps             | 2047     |
| time_exploring    | 79       |
--------------------------------
--------------------------------
| episodes          | 80       |
| mean_100ep_reward | -2.31    |
| steps             | 2750     |
| time_exploring    | 73       |
--------------------------------
--------------------------------
| episodes          | 100      |
| mean_100ep_reward | -2.47    |
| steps             | 3346     |
| time_exploring    | 67       |
--------------------------------
--------------------------------
| episodes          | 120      |
| mean_100ep_reward | -13.9    |
| steps             | 3846     |
| time_exploring    | 62       |
--------------------------------
--------------------------------
| episodes          | 140      |
| mean_100ep_reward | -25.7    |
| steps             | 4361     |
| time_exploring    | 57       |
--------------------------------
--------------------------------
| episodes          | 160      |
| mean_100ep_reward | -62.2    |
| steps             | 4657     |
| time_exploring    | 54       |
--------------------------------
--------------------------------
| episodes          | 180      |
| mean_100ep_reward | -81.7    |
| steps             | 5082     |
| time_exploring    | 50       |
--------------------------------
--------------------------------
| episodes          | 200      |
| mean_100ep_reward | -99.8    |
| steps             | 5553     |
| time_exploring    | 45       |
--------------------------------
--------------------------------
| episodes          | 220      |
| mean_100ep_reward | -106     |
| steps             | 5952     |
| time_exploring    | 41       |
--------------------------------
--------------------------------
| episodes          | 240      |
| mean_100ep_reward | -95      |
| steps             | 6415     |
| time_exploring    | 37       |
--------------------------------
--------------------------------
| episodes          | 260      |
| mean_100ep_reward | -57.4    |
| steps             | 6796     |
| time_exploring    | 33       |
--------------------------------
--------------------------------
| episodes          | 280      |
| mean_100ep_reward | -57.6    |
| steps             | 7209     |
| time_exploring    | 29       |
--------------------------------
--------------------------------
| episodes          | 300      |
| mean_100ep_reward | -39.3    |
| steps             | 7699     |
| time_exploring    | 24       |
--------------------------------
--------------------------------
| episodes          | 320      |
| mean_100ep_reward | -20.6    |
| steps             | 8037     |
| time_exploring    | 21       |
--------------------------------
--------------------------------
| episodes          | 340      |
| mean_100ep_reward | -19.7    |
| steps             | 8392     |
| time_exploring    | 17       |
--------------------------------
--------------------------------
| episodes          | 360      |
| mean_100ep_reward | -29.7    |
| steps             | 8755     |
| time_exploring    | 14       |
--------------------------------
--------------------------------
| episodes          | 380      |
| mean_100ep_reward | -35.8    |
| steps             | 9134     |
| time_exploring    | 10       |
--------------------------------
--------------------------------
| episodes          | 400      |
| mean_100ep_reward | -60.7    |
| steps             | 9510     |
| time_exploring    | 6        |
--------------------------------
--------------------------------
| episodes          | 420      |
| mean_100ep_reward | -104     |
| steps             | 9913     |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 440      |
| mean_100ep_reward | -130     |
| steps             | 10307    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 460      |
| mean_100ep_reward | -171     |
| steps             | 10711    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 480      |
| mean_100ep_reward | -216     |
| steps             | 11146    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 500      |
| mean_100ep_reward | -233     |
| steps             | 11525    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 520      |
| mean_100ep_reward | -270     |
| steps             | 11933    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 540      |
| mean_100ep_reward | -297     |
| steps             | 12330    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 560      |
| mean_100ep_reward | -306     |
| steps             | 12727    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 580      |
| mean_100ep_reward | -297     |
| steps             | 13135    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 600      |
| mean_100ep_reward | -335     |
| steps             | 13579    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 620      |
| mean_100ep_reward | -307     |
| steps             | 14006    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 640      |
| mean_100ep_reward | -289     |
| steps             | 14372    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 660      |
| mean_100ep_reward | -244     |
| steps             | 14778    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 680      |
| mean_100ep_reward | -228     |
| steps             | 15153    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 700      |
| mean_100ep_reward | -191     |
| steps             | 15538    |
| time_exploring    | 2        |
--------------------------------
