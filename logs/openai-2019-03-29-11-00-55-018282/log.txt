Logging to logs/openai-2019-03-29-11-00-55-018282
scaling actions by [1. 1.] before executing in env
Saving model due to mean reward increase: None -> -2.42290821529627
-----------------------------------
| duration           | 598        |
| epoch              | 1          |
| mean_actor_loss    | 0.8525252  |
| mean_critic_loss   | 3.2355945  |
| mean_epoch_rewards | -17.6      |
| mean_epoch_steps   | 119        |
| mean_q_value       | -0.8526143 |
| test_epoch_rewards | -2.42      |
| test_epoch_steps   | 120        |
| total_steps        | 5000       |
-----------------------------------
Saving model due to mean reward increase: -2.42290821529627 -> -1.6888978977024578
-----------------------------------
| duration           | 1.21e+03   |
| epoch              | 2          |
| mean_actor_loss    | 0.98788214 |
| mean_critic_loss   | 5.2190523  |
| mean_epoch_rewards | -16.9      |
| mean_epoch_steps   | 116        |
| mean_q_value       | -0.9878922 |
| test_epoch_rewards | -1.69      |
| test_epoch_steps   | 120        |
| total_steps        | 10000      |
-----------------------------------
Saving model due to mean reward increase: -1.6888978977024578 -> -1.310618504703047
-----------------------------------
| duration           | 1.83e+03   |
| epoch              | 3          |
| mean_actor_loss    | 0.9986341  |
| mean_critic_loss   | 4.1739774  |
| mean_epoch_rewards | -16.5      |
| mean_epoch_steps   | 108        |
| mean_q_value       | -0.9986344 |
| test_epoch_rewards | -1.31      |
| test_epoch_steps   | 121        |
| total_steps        | 15000      |
-----------------------------------
Saving model due to mean reward increase: -1.310618504703047 -> -1.0708814362198134
-----------------------------------
| duration           | 2.41e+03   |
| epoch              | 4          |
| mean_actor_loss    | 0.9998079  |
| mean_critic_loss   | 3.1870325  |
| mean_epoch_rewards | -14.3      |
| mean_epoch_steps   | 112        |
| mean_q_value       | -0.9998077 |
| test_epoch_rewards | -1.07      |
| test_epoch_steps   | 121        |
| total_steps        | 20000      |
-----------------------------------
Saving model due to mean reward increase: -1.0708814362198134 -> -1.0162126942372338
-----------------------------------
| duration           | 3.01e+03   |
| epoch              | 5          |
| mean_actor_loss    | 0.99997497 |
| mean_critic_loss   | 4.86903    |
| mean_epoch_rewards | -13.1      |
| mean_epoch_steps   | 118        |
| mean_q_value       | -0.999975  |
| test_epoch_rewards | -1.02      |
| test_epoch_steps   | 121        |
| total_steps        | 25000      |
-----------------------------------
------------------------------------
| duration           | 3.64e+03    |
| epoch              | 6           |
| mean_actor_loss    | 0.9999971   |
| mean_critic_loss   | 4.9111843   |
| mean_epoch_rewards | -15.3       |
| mean_epoch_steps   | 110         |
| mean_q_value       | -0.99999696 |
| test_epoch_rewards | -1.34       |
| test_epoch_steps   | 121         |
| total_steps        | 30000       |
------------------------------------
-----------------------------------
| duration           | 4.25e+03   |
| epoch              | 7          |
| mean_actor_loss    | 0.99999976 |
| mean_critic_loss   | 3.2003753  |
| mean_epoch_rewards | -16.3      |
| mean_epoch_steps   | 106        |
| mean_q_value       | -0.9999998 |
| test_epoch_rewards | -2.05      |
| test_epoch_steps   | 120        |
| total_steps        | 35000      |
-----------------------------------
-----------------------------------
| duration           | 4.87e+03   |
| epoch              | 8          |
| mean_actor_loss    | 0.99999994 |
| mean_critic_loss   | 3.7517297  |
| mean_epoch_rewards | -16.3      |
| mean_epoch_steps   | 105        |
| mean_q_value       | -0.9999999 |
| test_epoch_rewards | -1.98      |
| test_epoch_steps   | 120        |
| total_steps        | 40000      |
-----------------------------------
---------------------------------
| duration           | 5.49e+03 |
| epoch              | 9        |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 4.129649 |
| mean_epoch_rewards | -13.3    |
| mean_epoch_steps   | 106      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.87    |
| test_epoch_steps   | 120      |
| total_steps        | 45000    |
---------------------------------
---------------------------------
| duration           | 6.11e+03 |
| epoch              | 10       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 4.22624  |
| mean_epoch_rewards | -12.3    |
| mean_epoch_steps   | 109      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.8     |
| test_epoch_steps   | 120      |
| total_steps        | 50000    |
---------------------------------
----------------------------------
| duration           | 6.76e+03  |
| epoch              | 11        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 4.4952617 |
| mean_epoch_rewards | -13.9     |
| mean_epoch_steps   | 106       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.43     |
| test_epoch_steps   | 120       |
| total_steps        | 55000     |
----------------------------------
----------------------------------
| duration           | 7.39e+03  |
| epoch              | 12        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 3.9384072 |
| mean_epoch_rewards | -17.5     |
| mean_epoch_steps   | 105       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.56     |
| test_epoch_steps   | 121       |
| total_steps        | 60000     |
----------------------------------
----------------------------------
| duration           | 8.02e+03  |
| epoch              | 13        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 4.9888563 |
| mean_epoch_rewards | -13.4     |
| mean_epoch_steps   | 110       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.82     |
| test_epoch_steps   | 121       |
| total_steps        | 65000     |
----------------------------------
----------------------------------
| duration           | 8.65e+03  |
| epoch              | 14        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 5.0439286 |
| mean_epoch_rewards | -11       |
| mean_epoch_steps   | 108       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.78     |
| test_epoch_steps   | 121       |
| total_steps        | 70000     |
----------------------------------
----------------------------------
| duration           | 9.29e+03  |
| epoch              | 15        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 5.1187196 |
| mean_epoch_rewards | -14.8     |
| mean_epoch_steps   | 104       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -2.3      |
| test_epoch_steps   | 121       |
| total_steps        | 75000     |
----------------------------------
----------------------------------
| duration           | 9.95e+03  |
| epoch              | 16        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 6.3127165 |
| mean_epoch_rewards | -18.6     |
| mean_epoch_steps   | 101       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -2.07     |
| test_epoch_steps   | 121       |
| total_steps        | 80000     |
----------------------------------
----------------------------------
| duration           | 1.06e+04  |
| epoch              | 17        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 5.9868026 |
| mean_epoch_rewards | -17.4     |
| mean_epoch_steps   | 103       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.57     |
| test_epoch_steps   | 121       |
| total_steps        | 85000     |
----------------------------------
---------------------------------
| duration           | 1.12e+04 |
| epoch              | 18       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 4.053258 |
| mean_epoch_rewards | -15.6    |
| mean_epoch_steps   | 106      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.65    |
| test_epoch_steps   | 121      |
| total_steps        | 90000    |
---------------------------------
---------------------------------
| duration           | 1.18e+04 |
| epoch              | 19       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 6.276535 |
| mean_epoch_rewards | -13.1    |
| mean_epoch_steps   | 109      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.55    |
| test_epoch_steps   | 121      |
| total_steps        | 95000    |
---------------------------------
----------------------------------
| duration           | 1.24e+04  |
| epoch              | 20        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 5.8228655 |
| mean_epoch_rewards | -16.4     |
| mean_epoch_steps   | 109       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.17     |
| test_epoch_steps   | 122       |
| total_steps        | 100000    |
----------------------------------
----------------------------------
| duration           | 1.31e+04  |
| epoch              | 21        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 5.7070866 |
| mean_epoch_rewards | -16.5     |
| mean_epoch_steps   | 107       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.68     |
| test_epoch_steps   | 121       |
| total_steps        | 105000    |
----------------------------------
---------------------------------
| duration           | 1.37e+04 |
| epoch              | 22       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 4.833338 |
| mean_epoch_rewards | -14.9    |
| mean_epoch_steps   | 106      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.55    |
| test_epoch_steps   | 121      |
| total_steps        | 110000   |
---------------------------------
---------------------------------
| duration           | 1.43e+04 |
| epoch              | 23       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 5.115119 |
| mean_epoch_rewards | -12.8    |
| mean_epoch_steps   | 110      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.32    |
| test_epoch_steps   | 121      |
| total_steps        | 115000   |
---------------------------------
----------------------------------
| duration           | 1.49e+04  |
| epoch              | 24        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 4.5242634 |
| mean_epoch_rewards | -10.9     |
| mean_epoch_steps   | 112       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.87     |
| test_epoch_steps   | 120       |
| total_steps        | 120000    |
----------------------------------
---------------------------------
| duration           | 1.55e+04 |
| epoch              | 25       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 3.98433  |
| mean_epoch_rewards | -13      |
| mean_epoch_steps   | 114      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.42    |
| test_epoch_steps   | 120      |
| total_steps        | 125000   |
---------------------------------
---------------------------------
| duration           | 1.61e+04 |
| epoch              | 26       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 3.851167 |
| mean_epoch_rewards | -15.8    |
| mean_epoch_steps   | 108      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.46    |
| test_epoch_steps   | 119      |
| total_steps        | 130000   |
---------------------------------
---------------------------------
| duration           | 1.67e+04 |
| epoch              | 27       |
| mean_actor_loss    | 1.0      |
| mean_critic_loss   | 4.44349  |
| mean_epoch_rewards | -17.4    |
| mean_epoch_steps   | 105      |
| mean_q_value       | -1.0     |
| test_epoch_rewards | -1.71    |
| test_epoch_steps   | 120      |
| total_steps        | 135000   |
---------------------------------
----------------------------------
| duration           | 1.74e+04  |
| epoch              | 28        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 3.7075393 |
| mean_epoch_rewards | -16.7     |
| mean_epoch_steps   | 112       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.72     |
| test_epoch_steps   | 119       |
| total_steps        | 140000    |
----------------------------------
----------------------------------
| duration           | 1.8e+04   |
| epoch              | 29        |
| mean_actor_loss    | 1.0       |
| mean_critic_loss   | 4.7305856 |
| mean_epoch_rewards | -14.3     |
| mean_epoch_steps   | 113       |
| mean_q_value       | -1.0      |
| test_epoch_rewards | -1.91     |
| test_epoch_steps   | 119       |
| total_steps        | 145000    |
----------------------------------
