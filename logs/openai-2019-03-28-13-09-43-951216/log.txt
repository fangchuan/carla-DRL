Logging to logs/openai-2019-03-28-13-09-43-951216
 the network use 2200375.0 varibales
--------------------------------
| episodes          | 20       |
| mean_100ep_reward | 5.53     |
| steps             | 1019     |
| time_exploring    | 90       |
--------------------------------
Saving model due to mean reward increase: None -> 5.53
--------------------------------
| episodes          | 40       |
| mean_100ep_reward | 0.94     |
| steps             | 2039     |
| time_exploring    | 80       |
--------------------------------
--------------------------------
| episodes          | 60       |
| mean_100ep_reward | -7.35    |
| steps             | 3059     |
| time_exploring    | 70       |
--------------------------------
--------------------------------
| episodes          | 80       |
| mean_100ep_reward | -10.7    |
| steps             | 4079     |
| time_exploring    | 60       |
--------------------------------
--------------------------------
| episodes          | 100      |
| mean_100ep_reward | -14      |
| steps             | 5098     |
| time_exploring    | 50       |
--------------------------------
--------------------------------
| episodes          | 120      |
| mean_100ep_reward | -20.6    |
| steps             | 6118     |
| time_exploring    | 40       |
--------------------------------
--------------------------------
| episodes          | 140      |
| mean_100ep_reward | -25.1    |
| steps             | 7138     |
| time_exploring    | 30       |
--------------------------------
--------------------------------
| episodes          | 160      |
| mean_100ep_reward | -27.6    |
| steps             | 8158     |
| time_exploring    | 20       |
--------------------------------
--------------------------------
| episodes          | 180      |
| mean_100ep_reward | -33.4    |
| steps             | 9167     |
| time_exploring    | 10       |
--------------------------------
--------------------------------
| episodes          | 200      |
| mean_100ep_reward | -39.8    |
| steps             | 10138    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 220      |
| mean_100ep_reward | -47.4    |
| steps             | 11110    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 240      |
| mean_100ep_reward | -57.3    |
| steps             | 12036    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 260      |
| mean_100ep_reward | -62      |
| steps             | 12985    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 280      |
| mean_100ep_reward | -66.7    |
| steps             | 13870    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 300      |
| mean_100ep_reward | -69      |
| steps             | 14769    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 320      |
| mean_100ep_reward | -65.5    |
| steps             | 15754    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 340      |
| mean_100ep_reward | -64.8    |
| steps             | 16681    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 360      |
| mean_100ep_reward | -66.1    |
| steps             | 17647    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 380      |
| mean_100ep_reward | -65.9    |
| steps             | 18579    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 400      |
| mean_100ep_reward | -67.5    |
| steps             | 19497    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 420      |
| mean_100ep_reward | -72.6    |
| steps             | 20471    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 440      |
| mean_100ep_reward | -71.1    |
| steps             | 21456    |
| time_exploring    | 2        |
--------------------------------
--------------------------------
| episodes          | 460      |
| mean_100ep_reward | -71.3    |
| steps             | 22399    |
| time_exploring    | 2        |
--------------------------------
